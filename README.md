# BEE12-G1-Human-Action_Recognition-Through-Videos-Usman_Jalil
# Human Action Recognition Through Videos using Conv LSTMs

## Group Members
- **John Doe**
  - **Registration Number:** ABC123
- **Jane Smith**
  - **Registration Number:** XYZ456
- **Sam Johnson**
  - **Registration Number:** DEF789

## Abstract
This project focuses on implementing Human Action Recognition through Videos using Convolutional Long Short-Term Memory networks (Conv LSTMs). The goal is to develop a robust system that can accurately identify and classify human actions in video sequences. The significance of this work lies in its potential applications in various domains, including surveillance, human-computer interaction, and sports analytics.

## Methodology
Our approach involves the use of Convolutional LSTMs, which are well-suited for capturing both spatial and temporal features in video data. The project includes the following key steps:
1. **Data Collection:** Acquiring a diverse dataset of human actions in video format.
2. **Data Preprocessing:** Cleaning and formatting the data for training.
3. **Model Architecture:** Designing and implementing a Convolutional LSTM architecture for action recognition.
4. **Training:** Training the model on the prepared dataset.
5. **Evaluation:** Assessing the model's performance using appropriate metrics.
6. **Usage:** Providing a user-friendly interface for utilizing the trained model.

## Repository Structure
- **/data:** Contains the dataset used for training and testing.
- **/src:** Source code files for the Convolutional LSTM model.
- **/docs:** Documentation and project-related files.
- **/results:** Output and results generated during the project.

## Getting Started
To get started with the project, follow these steps:
1. Clone the repository: `git clone [repository_url]`
2. Navigate to the project directory: `cd human-action-recognition`
3. Install dependencies: `pip install -r requirements.txt`
4. Run the main script: `python main.py`

## Usage
After setting up the project, you can use the trained model to recognize human actions in videos. Adjust the input parameters and paths as needed in the `main.py` script. For more detailed instructions, refer to the [Usage documentation](docs/usage.md).

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

